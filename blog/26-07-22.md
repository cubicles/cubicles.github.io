[\[Return to blog\]](index.md) [\[Home\]](../index.md)

## Brainfood Blog: Delta Lake

Delta Lake es un framework de almacenamiento desarrollado en el lenguaje
de programación Scala. Es una herramienta de código abierto para la
arquitectura Lakehouse. Popular por sus integraciones con Spark, Flink,
PrestoDB, Hive, entre otros, así como por su capacidad para poder realizar
transacciones ACID. Ofrece un manejo escalable de metadata, versionado de
snapshots, variaciones de esquema, entre otros, convirtiéndola en una
herramienta robuesta para la construcción de Data Lakehouses.

### Arquitectura Lakehouse

#### Data Warehouse

En primer lugar, tenemos una arquitectura Data Warehouse, pensada para soportar
bloques de data estructurada y en dar soporte a las areas de reporteria y BI,
centrándose en la generación de valor para los tomadores de decisiones.
Esta arquitectura tiene un costo elevado para manejar data no estructurada o
semi-estructurada, así como dificultades para incrementar en volumen y usuarios
concurrentes.

#### Data Lake

En segundo lugar, consideremos una arquitectura Data Lake. Este sistema tiene
como base el almacenamiento de data cruda (Estructurada, semi y no estructurada)
y se centra en la disponibilización de la información para las áreas de
Data Science y Machine Learning. Si bien cubre deficiencias de un Data Warehouse,
no refuerzan la calidad de la data, consistencia, aislamiento y presenta
dificultades para realizar mezclas de jobs en batch y streaming.

#### Arquitectura Lakehouse

Una alternativa para cubrir los gaps de las dos previas arquitecturas mencionadas
es implementar ambos sistemas, con la finalidad de habilitar tanto al equipo
de reporteria y BI como a los data scientists y machine learning engineers.
Esta alternativa resulta en duplicidad de información y costos elevados de
infraestructura. En este contexto, se presenta la arquitectura Lakehouse, como
una alternativa que combina la flexibilidad y escalabilidad de los data lakes con
la transaccionabilidad de los data warehouses.

<p align="center">
    <img src="resources/data-lakehouse-new.png" alt="lakehouse" width="800px"/>
</p>

La arquitectura lakehouse implementa las funcionalidades de estructuración de
datos y de gestión de la información sobre la capa de bajo costo empleada en 
data lakes. Los diferentes equipos de data science, machine learning, BI y
reportería tendrán acceso directo a la versión mas reciente de la información,
como se muestra en la imagen.

Las funcionalidades clave que conforman la arquitectura lakehouse son:

-  Capas de metadata: Mediante el uso de archivos Parquet se almacena la metadata
que guarda el versionado de la información y ofrece transacciones ACID

-  Optimización de queries: Mediante el cacheo de data en RAM/SSDs y ejecuciones
vectoriales en CPUs modernos.

-  Acceso optimizado a herramientas de Data Science y Machine Learning: El uso
del formato Parquet y compatibilidad con herramientas populares como Pandas, 
TensorFlow, PyTorch, entre otros.

### Transacciones ACID

Que son transacciones ACID

## Delta Lake in cloud - Hands On

Como implementar un Lakehouse con las herramientas cloud para las 3 nubes

## Recomendaciones

En que casos usar lakehouse. Delta lake sobre arquitectura comun. 





